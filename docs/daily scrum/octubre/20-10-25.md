```markdown
---
layout: default
title: 20-10-25
parent: Daily Scrum
grand_parent: DevLog
---

# Daily Scrum - 20 de octubre de 2025

‚úÖ **Qu√© hice ayer**
- Complet√© el sistema RAG b√°sico (endpoints `/index` y `/search`).
- Prepar√© el entorno y la estructura para el scraper de gu√≠as docentes.

üéØ **Qu√© har√© hoy**
[x] Investigar la estructura de las gu√≠as docentes de la UGR: identificar patrones de URL, secciones relevantes y restricciones (`robots.txt`).
[x] Recolectar ~5 gu√≠as representativas en HTML para pruebas.
[x] Dise√±ar el esquema JSON de salida (metadatos: asignatura, curso, grado, profesorado; secciones: objetivos, contenidos, bibliograf√≠a, criterios).
[ ] Crear el esqueleto `tools/ugr_scraper.py` y anotar dependencias necesarias en `backend/requirements.txt`.

‚ö†Ô∏è **Bloqueadores / Riesgos**
- Confirmar permiso para scraping (revisar `robots.txt`) y comprobar conectividad desde contenedores.
- Variabilidad en HTML entre facultades que puede necesitar selectores espec√≠ficos o heur√≠sticas.

üìù **Notas / Convenciones**
- Usar un User-Agent identificable; respetar `robots.txt` y aplicar delays/retries (exponencial) ante 429.
- Guardar HTML crudo en `rag_service/documents/raw/ugr/` con metadatos (URL, timestamp, hash) y la versi√≥n parseada en `rag_service/documents/parsed/ugr/`.

---
