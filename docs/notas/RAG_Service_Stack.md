---
layout: default
title: RAG Service - Stack TecnolÃ³gico
parent: Notas
---

# RAG Service - Stack TecnolÃ³gico y Arquitectura

**Fecha de decisiÃ³n**: 14 de Octubre 2025  
**Sprint**: Sprint 2  
**Responsable**: Gabriel Francisco

---

## ğŸ¯ Objetivo

Implementar un servicio RAG (Retrieval-Augmented Generation) para el chatbot educativo que permita:
- BÃºsqueda semÃ¡ntica en documentos acadÃ©micos
- Filtrado por metadata (asignatura, tipo de documento, etc.)
- IntegraciÃ³n agÃ©ntica con el LLM principal
- Escalabilidad y mantenibilidad

---

## âœ… Decisiones TÃ©cnicas

### 1. Base de Datos Vectorial: **Qdrant**

**ElecciÃ³n**: Qdrant  
**Alternativa considerada**: ChromaDB

#### JustificaciÃ³n:
- âœ… **Performance superior** en bÃºsquedas con metadata filtering
- âœ… **Escalabilidad** - diseÃ±ado para producciÃ³n desde el inicio
- âœ… **Metadata filtering avanzado** - crucial para filtrar por asignatura
- âœ… **API gRPC y HTTP** - flexibilidad en la comunicaciÃ³n
- âœ… **Persistencia robusta** - mejor manejo de datos en volumen
- âœ… **DocumentaciÃ³n excelente** y comunidad activa

#### Despliegue:
```yaml
# Contenedor Podman independiente
qdrant:
  image: qdrant/qdrant:latest
  container_name: qdrant-service
  ports:
    - "6333:6333"  # HTTP API
    - "6334:6334"  # gRPC API
  volumes:
    - ./qdrant_storage:/qdrant/storage
```

**Links**:
- [Qdrant Documentation](https://qdrant.tech/documentation/)
- [Qdrant Podman Setup](https://qdrant.tech/documentation/guides/installation/#docker)

---

### 2. Embeddings: **Ollama + nomic-embed-text**

**ElecciÃ³n**: Ollama con modelo `nomic-embed-text`  
**Alternativas consideradas**: Sentence Transformers (all-MiniLM-L6-v2), OpenAI embeddings

#### Especificaciones del modelo:
- **ParÃ¡metros**: 137M
- **Dimensionalidad**: 768
- **Contexto mÃ¡ximo**: 8192 tokens
- **Ventajas**:
  - âœ… Optimizado para bÃºsqueda semÃ¡ntica
  - âœ… Funciona completamente local/offline
  - âœ… Excelente relaciÃ³n calidad/recursos
  - âœ… IntegraciÃ³n sencilla con Ollama
  - âœ… Sin costes de API externa

#### Despliegue:
```yaml
# Contenedor Docker independiente
ollama:
  image: ollama/ollama:latest
  container_name: ollama-service
  ports:
    - "11434:11434"
  volumes:
    - ./ollama_models:/root/.ollama
```

**Comando para descargar el modelo**:
```bash
podman exec -it ollama-service ollama pull nomic-embed-text
```

**Links**:
- [Nomic Embed Text - Ollama](https://ollama.com/library/nomic-embed-text)
- [Nomic AI Documentation](https://docs.nomic.ai/)

---

### 3. Framework RAG: **LangChain**

**ElecciÃ³n**: LangChain  
**Alternativas consideradas**: LlamaIndex, implementaciÃ³n custom

#### JustificaciÃ³n:
- âœ… **Ya integrado en el proyecto** - usamos LangChain + LangGraph
- âœ… **IntegraciÃ³n nativa con Qdrant** (`langchain-qdrant`)
- âœ… **Soporte para Ollama embeddings** (`langchain-ollama`)
- âœ… **RAG agÃ©ntico** - fÃ¡cil integraciÃ³n como herramienta del agente
- âœ… **Metadata filtering** - soporte directo para filtros
- âœ… **Ecosistema maduro** - muchos ejemplos y documentaciÃ³n

#### Dependencias necesarias:
```python
# requirements.txt para rag-service
langchain==0.3.0
langchain-qdrant==0.2.0
langchain-ollama==0.2.0
langchain-core==0.3.0
qdrant-client==1.11.0
```

**Links**:
- [LangChain Qdrant Integration](https://python.langchain.com/docs/integrations/vectorstores/qdrant)
- [LangChain Ollama Embeddings](https://python.langchain.com/docs/integrations/text_embedding/ollama)

---

## ğŸ—ï¸ Arquitectura del Sistema

### Diagrama de Componentes

```
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚                   Backend API                   â”‚
â”‚              (FastAPI + LangGraph)              â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
                    â”‚
                    â”‚ Agente llama herramienta RAG
                    â†“
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚                 RAG Service                     â”‚
â”‚           (Microservicio Python)                â”‚
â”‚                                                 â”‚
â”‚  â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”     â”‚
â”‚  â”‚  LangChain RAG Pipeline                â”‚     â”‚
â”‚  â”‚  - Query processing                    â”‚     â”‚
â”‚  â”‚  - Metadata filtering                  â”‚     â”‚
â”‚  â”‚  - Semantic search                     â”‚     â”‚
â”‚  â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜     â”‚
â””â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
       â”‚                            â”‚
       â”‚ Embeddings                 â”‚ Vector search
       â†“                            â†“
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”         â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚  Ollama Service â”‚         â”‚ Qdrant Service  â”‚
â”‚  nomic-embed-   â”‚         â”‚  Vector Store   â”‚
â”‚  text model     â”‚         â”‚  + Metadata     â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜         â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
```

### Flujo de Datos

1. **Usuario** hace pregunta al chatbot
2. **Agente (LangGraph)** decide si necesita contexto de documentos
3. **Herramienta RAG** es llamada con:
   - Query del usuario
   - Metadata filters (ej: `asignatura="LÃ³gica Difusa"`)
4. **RAG Service**:
   - Genera embedding de la query con Ollama
   - Busca en Qdrant con filtros de metadata
   - Retorna documentos relevantes + metadata
5. **Agente** usa el contexto para generar respuesta

---

## ğŸ“‹ Metadata Schema

Cada documento indexado tendrÃ¡ la siguiente metadata:

```python
{
    "asignatura": str,          # Ej: "LÃ³gica Difusa", "Ãlgebra"
    "tipo_documento": str,      # Ej: "apuntes", "ejercicios", "examen"
    "fecha": str,               # ISO format: "2025-10-14"
    "autor": str,               # Opcional
    "tema": str,                # Ej: "Conjuntos difusos"
    "fuente": str,              # Ej: "PRADO UGR", "Wikipedia"
    "chunk_id": int,            # ID del chunk dentro del documento
}
```

---

## ğŸš€ PrÃ³ximos Pasos

### Fase 1: Setup BÃ¡sico
- [ ] Actualizar `docker-compose.yml` con Qdrant y Ollama
- [ ] Crear estructura de carpetas `rag-service/`
- [ ] Configurar requirements.txt

### Fase 2: ImplementaciÃ³n Core
- [ ] Implementar cliente Qdrant con LangChain
- [ ] Implementar generaciÃ³n de embeddings con Ollama
- [ ] Crear pipeline de indexaciÃ³n de documentos
- [ ] Implementar bÃºsqueda con metadata filtering

### Fase 3: IntegraciÃ³n AgÃ©ntica
- [ ] Crear herramienta RAG para el agente (LangGraph)
- [ ] AÃ±adir a `backend/logic/tools.py`
- [ ] Actualizar System Prompt para uso de RAG

### Fase 4: Testing y OptimizaciÃ³n
- [ ] Tests unitarios del RAG service
- [ ] Tests de integraciÃ³n con el agente
- [ ] Benchmarking de performance
- [ ] Ajuste de parÃ¡metros (top_k, similarity threshold)

---

## ğŸ“š Referencias Adicionales

- [Building RAG with LangChain and Qdrant](https://qdrant.tech/documentation/tutorials/rag/)
- [Ollama Python Library](https://github.com/ollama/ollama-python)
- [LangChain RAG Tutorial](https://python.langchain.com/docs/tutorials/rag/)
- [Metadata Filtering Best Practices](https://qdrant.tech/documentation/concepts/filtering/)

---

## ğŸ” Consideraciones de Recursos

### EstimaciÃ³n de uso:
- **Qdrant**: ~100MB base + tamaÃ±o de documentos
- **Ollama (nomic-embed-text)**: ~274MB modelo descargado
- **RAM**: ~1-2GB para operaciÃ³n normal del RAG service

### Compatibilidad con recursos actuales:
- âœ… Compatible con GPU actual (usada por vLLM)
- âœ… Ollama puede usar CPU para embeddings
- âœ… Qdrant es ligero en recursos
- âš ï¸ Validar espacio en disco para persistencia

