# services:
#   vllm-main:
#     container_name: vllm-main
#     image: docker.io/vllm/vllm-openai:latest 
#     restart: unless-stopped
#     env_file:
#       - .env 
#     environment:
#       - HUGGING_FACE_HUB_TOKEN=${HF_TOKEN}
#       - VLLM_LOGGING_LEVEL=DEBUG
#       - NVIDIA_VISIBLE_DEVICES=all
#     ports:
#       - "8000:8000"
#     volumes:
#       - "./models:/models:rw" 
#     healthcheck:
#       test: ["CMD-SHELL", "curl -sSf http://localhost:8000/ || exit 1"]
#       interval: 30s
#       timeout: 5s
#       retries: 3
#       start_period: 15s
#     logging:
#       driver: "json-file"
#       options:
#         max-size: "50m"
#         max-file: "3"
#     tmpfs:
#       - /tmp
#     command: >
#       --model /models/phi-3-mini
#       --max-num-seqs 8
#       --max-model-len 1000
#       --quantization awq_marlin
#       --gpu-memory-utilization 0.7
#       --port 8000

services:
  vllm-openai:
    container_name: my-vllm-service
    deploy:
      resources:
        reservations:
          devices:
            - driver: nvidia
              device_ids: ['0']
              capabilities:
                - gpu
    volumes:
        - "./models:/models"
    env_file:
      - .env 
    environment:
        HUGGING_FACE_HUB_TOKEN: ${HF_TOKEN}
    ports:
        - 8000:8000
    image: docker.io/vllm/vllm-openai:latest
    command: >
      --model /models/TinyLlama--TinyLlama-1.1B-Chat-v1.0
      --max-num-seqs 8
      --max-model-len 1000
      --gpu-memory-utilization 0.7
      --port 8000
#     --quantization awq_marlin
